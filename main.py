import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import json
from datetime import datetime

# è¨­å®šç’°å¢ƒè®Šæ•¸ (åœ¨ GitHub Actions è£¡è¨­å®š)
TEAMS_WEBHOOK_URL = os.environ.get("TEAMS_WEBHOOK_URL")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# åˆå§‹åŒ– OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)
print 
def fetch_labor_laws():
    """
    ç¯„ä¾‹ï¼šçˆ¬å–å‹å‹•éƒ¨ã€Œæœ€æ–°æ¶ˆæ¯ã€æˆ–ã€Œæ³•è¦ç•°å‹•ã€
    é€™è£¡ä»¥æ¨¡æ“¬é‚è¼¯ç‚ºä¸»ï¼Œå¯¦éš› URL éœ€ä¾ç…§å‹å‹•éƒ¨ç•¶ä¸‹æ”¹ç‰ˆç‹€æ³èª¿æ•´
    """
    # é€™æ˜¯å‹å‹•éƒ¨æœ€æ–°æ¶ˆæ¯çš„ç¯„ä¾‹ç¶²å€ (éœ€æ ¹æ“šå¯¦éš›ç‹€æ³æ›¿æ›)
    url = "https://www.mol.gov.tw/1607/1632/1633/lpsimplelist" 
    
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_list = []
        # å‡è¨­æ–°èåˆ—è¡¨åœ¨ class="list" è£¡é¢ï¼Œé€™éƒ¨åˆ†éœ€è¦é‡å°ç›®æ¨™ç¶²ç«™æŒ‰ F12 è§€å¯Ÿ
        # é€™è£¡åƒ…ç‚ºå½ä»£ç¢¼é‚è¼¯
        for item in soup.select('.list a'):
            title = item.text.strip()
            link = item['href']
            date = item.find_next_sibling('span').text if item.find_next_sibling('span') else ""
            
            # é—œéµå­—éæ¿¾
            if "å‹å‹•åŸºæº–æ³•" in title or "å‹åŸºæ³•" in title:
                # æª¢æŸ¥æ˜¯å¦ç‚ºè¿‘æœŸçš„ (ä¾‹å¦‚ä»Šå¤©çš„æ—¥æœŸ)
                # é€™è£¡ç‚ºäº†æ¼”ç¤ºï¼Œå…ˆå…¨éƒ¨æŠ“ä¸‹ä¾†
                news_list.append({
                    "title": title,
                    "url": "https://www.mol.gov.tw" + link, # è£œå…¨ç¶²å€
                    "date": date
                })
        return news_list
    except Exception as e:
        print(f"çˆ¬èŸ²éŒ¯èª¤: {e}")
        return []

def analyze_with_gpt(news_item):
    """
    ä½¿ç”¨ GPT åˆ†ææ³•è¦å…§å®¹
    """
    prompt = f"""
    ä½ æ˜¯å°ç£å‹å‹•æ³•è¦å°ˆå®¶ã€‚è«‹åˆ†æä»¥ä¸‹é€™å‰‡é—œæ–¼å‹åŸºæ³•çš„è®Šå‹•é€šçŸ¥ï¼š
    æ¨™é¡Œï¼š{news_item['title']}
    é€£çµï¼š{news_item['url']}

    è«‹å¹«æˆ‘ç¸½çµä»¥ä¸‹è³‡è¨Š (è«‹ç”¨æ¢åˆ—å¼ï¼Œç¹é«”ä¸­æ–‡)ï¼š
    1. **è®Šå‹•æ‘˜è¦**ï¼šç°¡å–®èªªæ˜æ”¹äº†ä»€éº¼ï¼Ÿ
    2. **å½±éŸ¿å°è±¡**ï¼šèª°æœƒå—åˆ°å½±éŸ¿ï¼ˆé›‡ä¸»/å‹å·¥/ç‰¹å®šè¡Œæ¥­ï¼‰ï¼Ÿ
    3. **è¡Œå‹•å»ºè­°**ï¼šHR æˆ–å·¥ç¨‹å¸«éœ€è¦é…åˆåšä»€éº¼èª¿æ•´å—ï¼Ÿ
    """

    response = client.chat.completions.create(
        model="gpt-4o", # æˆ– gpt-3.5-turbo
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ³•å¾‹åˆ†æåŠ©æ‰‹ï¼Œè² è²¬æ•´ç†æ³•è¦è®Šæ›´é€šçŸ¥ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

def send_to_teams(summary, news_item):
    """
    ç™¼é€ Adaptive Card æˆ–ç°¡å–®è¨Šæ¯åˆ° Teams
    """
    # ç°¡å–®çš„ JSON æ ¼å¼
    payload = {
        "title": f"ğŸš¨ å‹åŸºæ³•è¦ç•°å‹•è­¦å ± - {news_item['date']}",
        "text": f"### [{news_item['title']}]({news_item['url']})\n\n{summary}"
    }
    
    headers = {'Content-Type': 'application/json'}
    print "test teams"
    print(f"payload: {data}") 
    print(f"TEAMS_WEBHOOK_URL: {TEAMS_WEBHOOK_URL}") 
    print requests.post(TEAMS_WEBHOOK_URL, data=json.dumps(payload), headers=headers)
    response = requests.post(TEAMS_WEBHOOK_URL, data=json.dumps(payload), headers=headers)
    
    if response.status_code == 200:
        print("è¨Šæ¯ç™¼é€æˆåŠŸ")
    else:
        print(f"è¨Šæ¯ç™¼é€å¤±æ•—: {response.status_code}")

def main():
    print("é–‹å§‹æª¢æŸ¥æ³•è¦è®Šå‹•...")
    news_items = fetch_labor_laws()
    
    if not news_items:
        print("ä»Šæ—¥ç„¡ç›¸é—œæ³•è¦è®Šå‹•ã€‚")
        return

    # ç‚ºäº†é¿å…é‡è¤‡ç™¼é€ï¼Œå¯¦éš›å°ˆæ¡ˆé€šå¸¸æœƒè¨˜éŒ„ã€Œå·²ç™¼é€éçš„æ¸…å–®ã€åœ¨ä¸€å€‹æª”æ¡ˆæˆ–è³‡æ–™åº«
    # é€™è£¡å‡è¨­æ¯æ¬¡éƒ½åˆ†ææœ€æ–°çš„ç¬¬ä¸€ç­†
    latest_news = news_items[0] 
    
    print(f"ç™¼ç¾æ–°è: {latest_news['title']}")
    analysis = analyze_with_gpt(latest_news)
    send_to_teams(analysis, latest_news)

if __name__ == "__main__":
    main()
